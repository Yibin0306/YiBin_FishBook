"""
3层神经网络：输入层（第0层）有 2个神经元，第1个隐藏层（第1层）有 3个神经元，
第2个隐藏层（第2层）有 2个神经元，输出层（第3层）有 2个神经元
"""

import numpy as np

# 符号确认
"""
神经网络的运算可以作为矩阵运算打包进行。因为神经网络各层的运算是通过矩阵的乘法运算打包进行的。
"""

def sigmoid(x):
    """Sigmoid激活函数实现（S形曲线）"""
    return 1 / (1 + np.exp(-x))

# === 演示神经网络各层间的信号传递过程 ===
# 输入层数据（2个特征）
X = np.array([1.0, 0.5])

# 第一层权重矩阵 (2x3)：连接输入层(2个神经元)到第一隐藏层(3个神经元)
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
# 第一层偏置向量 (3个元素)
B1 = np.array([0.1, 0.2, 0.3])

# 验证矩阵维度
print(W1.shape)  # (2, 3)  # 权重矩阵：2输入 x 3输出
print(X.shape)  # (2,)   # 输入向量：2个特征
print(B1.shape)  # (3,)   # 偏置向量：3个神经元

# 第一层加权输入计算：X·W1 + B1
A1 = np.dot(X, W1) + B1
print(A1)  # 显示加权和结果

# 应用激活函数得到第一层输出
Z1 = sigmoid(A1)
print(Z1)  # 显示激活后的输出

# 第二层权重矩阵 (3x2)：连接第一隐藏层(3个神经元)到第二隐藏层(2个神经元)
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
# 第二层偏置向量 (2个元素)
B2 = np.array([0.1, 0.2])

# 验证矩阵维度
print(Z1.shape)  # (3,)   # 第一层输出向量：3个神经元
print(W2.shape)  # (3,2)  # 权重矩阵：3输入 x 2输出
print(B2.shape)  # (2,)   # 偏置向量：2个神经元

# 第二层加权输入计算：Z1·W2 + B2
A2 = np.dot(Z1, W2) + B2
# 应用激活函数得到第二层输出
Z2 = sigmoid(A2)
print(A2)  # 显示加权和结果
print(Z2)  # 显示激活后的输出


# 恒等函数（用于输出层）
def identity_function(x):
    """回归问题使用的输出层激活函数（直接返回输入值）"""
    return x


# 第三层权重矩阵 (2x2)：连接第二隐藏层(2个神经元)到输出层(2个神经元)
W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
# 第三层偏置向量 (2个元素)
B3 = np.array([0.1, 0.2])

# 第三层加权输入计算：Z2·W3 + B3
A3 = np.dot(Z2, W3) + B3
# 应用输出层激活函数
Y = identity_function(A3)  # 对于回归问题使用恒等函数
print(Y)  # 显示最终输出结果

# === 网络结构封装实现 ===
"""
输出层的激活函数用σ()表示，不同于隐藏层的激活函数h()（σ读作sigma）
输出层的激活函数σ()根据问题类型选择：
- 回归问题：恒等函数
- 二元分类：sigmoid函数
- 多元分类：softmax函数
"""

# 代码实现小结
# init_network()函数会进行权重和偏置的初始化，并将它们保存在字典变量network中
def init_network():
    """初始化网络参数（权重和偏置）并返回字典"""
    network = {}
    # 第一层参数
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])  # 输入层→第一隐藏层
    network['B1'] = np.array([0.1, 0.2, 0.3])  # 第一隐藏层偏置
    # 第二层参数
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])  # 第一隐藏层→第二隐藏层
    network['B2'] = np.array([0.1, 0.2])  # 第二隐藏层偏置
    # 第三层参数
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])  # 第二隐藏层→输出层
    network['B3'] = np.array([0.1, 0.2])  # 输出层偏置
    return network

# forward()函数中则封装了将输入信号转换为输出信号的处理过程
# 这里出现了forward（前向）一词，它表示的是从输入到输出方向的传递处理。
def forward(network, x):
    """前向传播：计算从输入到输出的网络预测"""
    # 从网络字典中提取参数
    w1, w2, w3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['B1'], network['B2'], network['B3']

    # 第一层计算
    a1 = np.dot(x, w1) + b1  # 加权和
    z1 = sigmoid(a1)  # 激活函数

    # 第二层计算
    a2 = np.dot(z1, w2) + b2  # 加权和
    z2 = sigmoid(a2)  # 激活函数

    # 输出层计算
    a3 = np.dot(z2, w3) + b3  # 加权和
    y = identity_function(a3)  # 输出层激活（恒等函数）

    return y

# 初始化网络并执行前向传播
network = init_network()
x = np.array([1.0, 0.5])  # 输入样本
y = forward(network, x)  # 获得预测输出
print(y)  # 打印最终结果