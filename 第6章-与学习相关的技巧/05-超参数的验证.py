"""
超参数是指，比如各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等。
如果这些超参数没有设置合适的值，模型的性能就会很差。

超参数定义：
  需要人工设定的关键参数，包括：
  - 网络结构参数：神经元数量、层数
  - 优化参数：学习率(lr)、权值衰减系数(weight_decay)
  - 训练参数：批量大小(batch_size)、迭代次数(epochs)
"""

# 验证数据
"""
不能使用测试数据评估超参数的性能。这一点非常重要，但也容易被忽视。
用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。
这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。

调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。
"""

# 超参数的最优化
"""
步骤0
    设定超参数的范围。
步骤1
    从设定的超参数范围中随机采样。
步骤2
    使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。
步骤3
    重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。

反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。
这就是进行超参数的最优化的一种方法。
"""

# 超参数最优化的实现
import numpy as np
# 权值衰减系数采样：10^{-8}到10^{-4}指数范围
weight_decay = 10 ** np.random.uniform(-8, -4)
"""
权值衰减(weight_decay)搜索说明：
  - 典型范围：10^{-8}到10^{-4}
  - 使用指数均匀分布(np.random.uniform)原因：
      * 权值衰减的有效范围跨越多个数量级
      * 对数尺度采样更符合参数敏感度分布
  - 数学等价：在log10(weight_decay)上均匀采样
"""

# 学习率采样：10^{-6}到10^{-2}指数范围
lr = 10 ** np.random.uniform(-6, -2)
"""
学习率(lr)搜索说明：
  - 典型范围：10^{-6}到10^{-2}
  - 指数采样原因同上
  - 优化建议：学习率与批量大小耦合
      batch_size增大k倍 → 学习率相应增大√k倍
"""

"""
常见超参数经验范围：
| 超参数        | 典型范围/值           | 备注                     |
|---------------|----------------------|-------------------------|
| 学习率(lr)    | 10^{-5} - 10^{-2}    | 常用3e-4, 1e-3          |
| 权值衰减      | 10^{-8} - 10^{-3}    | CNN常用5e-4, Transformer常用0.01 |
| 批量大小      | 32-1024              | GPU显存决定上限          |
| Dropout比例   | 0.1-0.7              | 输入层低，全连接层高      |
| 优化器选择    | Adam, SGD, RMSprop   | Adam为默认推荐           |
"""