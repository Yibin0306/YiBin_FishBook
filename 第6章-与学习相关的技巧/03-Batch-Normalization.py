# Batch Normalization 的算法
"""
Batch Norm有以下优点。
• 可以使学习快速进行（可以增大学习率）。
• 不那么依赖初始值（对于初始值不用那么神经质）。
• 抑制过拟合（降低Dropout等的必要性）。

具体而言，就是进行使数据分布的均值为0、方差为1的正规化。

# 计算小批量均值
μ_B = (1/m) * Σ_{i=1}^{m} x_i

# 计算小批量方差
σ²_B = (1/m) * Σ_{i=1}^{m} (x_i - μ_B)^2

# 标准化操作
x̂_i = (x_i - μ_B) / √(σ²_B + ε)

# 缩放和平移操作
y_i = γ * x̂_i + β

其中：
γ（gamma）是缩放参数（可学习）
β（beta）是平移参数（可学习）

通过使用Batch Norm，可以推动学习的进行。
并且，对权重初始值变得健壮（“对初始值健壮”表示不那么依赖初始值）。
Batch Norm具备了如此优良的性质，一定能应用在更多场合中。
"""

